{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Data Pre-processing:](#toc1_)    \n",
    "  - [Import Libraries and Datasets](#toc1_1_)    \n",
    "  - [Formatting](#toc1_2_)    \n",
    "  - [Examining Data:](#toc1_3_)    \n",
    "    - [Columns and Data Types](#toc1_3_1_)    \n",
    "    - [Dropping Non-Required Columns](#toc1_3_2_)    \n",
    "    - [Checking/Handling Missing Values](#toc1_3_3_)    \n",
    "- [Feature Engineering:](#toc2_)    \n",
    "  - [Merchant Characteristics](#toc2_1_)    \n",
    "  - [Loan Characteristics](#toc2_2_)    \n",
    "  - [Transaction Characteristics](#toc2_3_)    \n",
    "  - [Loan-Sales Ratios](#toc2_4_)    \n",
    "  - [Target Variables](#toc2_5_)    \n",
    "- [Data Exploration:](#toc3_)    \n",
    "  - [Checking/Handling Missing Values](#toc3_1_)    \n",
    "  - [Checking/Removing Outliers](#toc3_2_)    \n",
    "- [Models:](#toc4_)    \n",
    "  - [Splitting Dataset into Training and Testing](#toc4_1_)    \n",
    "  - [Model Selection and Training:](#toc4_2_)    \n",
    "    - [XGBoost](#toc4_2_1_)    \n",
    "    - [Decision Tree and Random Forest](#toc4_2_2_)    \n",
    "  - [Hypertuning](#toc4_3_)    \n",
    "- [Notes:](#toc4_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a> Data Pre-processing:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a> Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing \n",
    "import seaborn as sns\n",
    "import xgboost as xgb # type: ignore\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchantDetails = pd.DataFrame({\n",
    "\n",
    "})\n",
    "\n",
    "loanScheduleDetails = pd.DataFrame({\n",
    " \n",
    "})\n",
    "\n",
    "loanLedgerDetails = pd.DataFrame({\n",
    " \n",
    "})\n",
    "\n",
    "transactionDetails = pd.DataFrame({\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a> Formatting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns with same name\n",
    "\n",
    "merchantDetails.rename(columns={'id': 'merchant_code', 'created_at': 'merchant_created_at', 'updated_at':'merchant_updated_at'}, inplace=True)\n",
    "\n",
    "loanScheduleDetails.rename(columns={'id': 'loan_schedule_id', 'created_at': 'loan_schedule_created_at', 'updated_at':'loan_schedule_updated_at', 'paid_date':'loan_schedule_paid_date'}, inplace=True)\n",
    "\n",
    "loanLedgerDetails.rename(columns={'id': 'loan_id', 'created_at': 'loan_created_at', 'updated_at':'loan_updated_at', 'deleted_at':'loan_deleted_at', 'transaction_date':'loan_transaction_date', 'transaction_type':'loan_transaction_type', 'description':'loan_description'}, inplace=True)\n",
    "\n",
    "transactionDetails.rename(columns={'id': 'transaction_id', 'created_at': 'transaction_created_at'}, inplace=True)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "\n",
    "merchantDetails[['merchant_created_at', 'merchant_updated_at']] = merchantDetails.apply(pd.to_datetime)\n",
    "\n",
    "loanScheduleDetails[['loan_schedule_created_at', 'loan_schedule_updated_at', 'loan_schedule_paid_date', 'schedule_date']] = loanScheduleDetails.apply(pd.to_datetime)\n",
    "\n",
    "loanLedgerDetails[['loan_created_at', 'loan_updated_at', 'loan_deleted_at']] = merchantDetails.apply(pd.to_datetime)\n",
    "\n",
    "transactionDetails[['transaction_datetime', 'transaction_created_at']] = transactionDetails.apply(pd.to_datetime)\n",
    "\n",
    "# Use loans only older than 3 months\n",
    "\n",
    "# Calculate the cutoff date for loans older than 3 months\n",
    "cutoff_date = pd.Timestamp.today() - pd.DateOffset(months=3)\n",
    "\n",
    "loanSchedule = loanScheduleDetails[loanScheduleDetails['schedule_date'] < cutoff_date]\n",
    "\n",
    "loanLedger = loanLedgerDetails[loanLedgerDetails['loan_created_at'] < cutoff_date]\n",
    "\n",
    "# Merge the data on 'merchant_code' to get only merchants who have or have had loans\n",
    "\n",
    "merchants = pd.merge(merchantDetails, loanSchedule[['merchant_code']], on='merchant_code', how='inner')\n",
    "\n",
    "# Filter transactionDetails to only include merchants who have taken loans and transactions prior to loan disbursement\n",
    "\n",
    "transactions = pd.merge(transactionDetails, loanSchedule[['merchant_code', 'loan_schedule_created_at']], on='merchant_code', how='inner')\n",
    "\n",
    "transactions = transactions[transactions['transaction_created_at'] < transactions['loan_schedule_created_at']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a> Examining Data:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a> Columns and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants.head()\n",
    "loanSchedule.head()\n",
    "loanLedger.head()\n",
    "transactions.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants.info()\n",
    "loanSchedule.info()\n",
    "loanLedger.info()\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a> Dropping Non-Required Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants=merchants.drop([''],axis=1)\n",
    "loanSchedule=loanSchedule.drop([''],axis=1)\n",
    "loanLedger=loanSchedule.drop([''],axis=1)\n",
    "transactions=transactions.drop([''],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a> Checking/Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info_merchants = merchants.isnull().sum()\n",
    "missing_info_loanSchedule = loanSchedule.isnull().sum()\n",
    "missing_info_loanLedger = loanLedger.isnull().sum()\n",
    "missing_info_transactions = transactions.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a> Feature Engineering:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a> Merchant Characteristics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merch_char = pd.DataFrame()\n",
    "merch_char['merchant_code'] = merchants['merchant_code']\n",
    "\n",
    "# Age of relationship with the payment company at the time of disbursal (in months)\n",
    "\n",
    "disbursement_df = loanLedger[loanLedger['loan_transaction_type'] == 'DISBURSEMENT']\n",
    "merchant_creation_map = merchants.set_index('merchant_code')['merchant_created_at'].to_dict()\n",
    "disbursement_df['merchant_created_at'] = disbursement_df['merchant_code'].map(merchant_creation_map)\n",
    "disbursement_df['relationage'] = (disbursement_df['loan_transaction_date'] - disbursement_df['merchant_created_at']).dt.month\n",
    "disbursement_age_df = disbursement_df[['merchant_code', 'relationage']]\n",
    "\n",
    "merch_char = pd.merge(merch_char, disbursement_age_df, on='merchant_code', how='left')\n",
    "\n",
    "# Age of the business owner at the time of the payment company loan\n",
    "# Gender (Dummy variable assigning Male = 1)\n",
    "\n",
    "# (For repeat borrowers) Number of days between disbursal of the repeat loan and closure of the previous loan\n",
    "# (For repeat borrowers) Principal amount of the previous loan\n",
    "# (For repeat borrowers) implied days past due for previous loan\n",
    "# (For repeat borrowers) 1 if the previous loan was late on implied criteria (lastidpd > 30)\n",
    "# (For repeat borrowers) Loan number (first loan, second loan, . . . )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a> Loan Characteristics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_char = pd.DataFrame()\n",
    "loan_char['merchant_code'] = merchants['merchant_code']\n",
    "\n",
    "# Month of loan disbursal\n",
    "\n",
    "disbursement_df['mdisburse'] = disbursement_df['loan_transaction_date'].dt.month\n",
    "month_of_disbursement = disbursement_df[['merchant_code', 'mdisburse']]\n",
    "loan_char = pd.merge(loan_char, month_of_disbursement, on='merchant_code', how='left')\n",
    "\n",
    "# Principal amount\n",
    "\n",
    "loan_char = pd.merge(loan_char, disbursement_df[['merchant_code', 'credit']], on='merchant_code', how='left')\n",
    "loan_char.rename(columns={'credit': 'loanamount'}, inplace=True)\n",
    "\n",
    "# Suggested tenure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a> Transaction Characteristics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trans_char(transactions, sales_period_end=1, sales_period_start=28, suffix=''):\n",
    "\n",
    "    # Merge disbursement date to transactions\n",
    "    disbursement_dates_amounts = loanLedger[loanLedger['loan_transaction_type'] == 'DISBURSEMENT']\n",
    "\n",
    "    transactions = pd.merge(transactions, disbursement_dates_amounts[['merchant_code', 'loan_transaction_date', 'debit']], on='merchant_code', how='left')\n",
    "    results = []\n",
    "\n",
    "    # Loop for each merchant\n",
    "    for merchant_code in transactions['merchant_code'].unique():\n",
    "\n",
    "        # Filter transactions for the specific merchant\n",
    "        merchant_transactions = transactions[transactions['merchant_code'] == merchant_code]\n",
    "        disbursement_date = merchant_transactions['loan_transaction_date'].iloc[0]\n",
    "\n",
    "        # Filter transactions for the sales period before disbursement\n",
    "        sales_period_transactions_all = merchant_transactions[\n",
    "            (merchant_transactions['transaction_datetime'] >= (disbursement_date - pd.Timedelta(days=sales_period_start))) &\n",
    "            (merchant_transactions['transaction_datetime'] <= (disbursement_date - pd.Timedelta(days=sales_period_end)))\n",
    "        ]\n",
    "\n",
    "        # Filter transactions by response_code = 0\n",
    "        sales_period_transactions = sales_period_transactions_all[sales_period_transactions_all['response_code'] == 0]\n",
    "\n",
    "       # Aggregate sales by date\n",
    "        daily_sales = sales_period_transactions.groupby(sales_period_transactions['transaction_datetime'].dt.date)['amount'].sum().reset_index(name='daily_sales')\n",
    "        daily_sales['transaction_datetime'] = pd.to_datetime(daily_sales['transaction_datetime'])\n",
    "\n",
    "        # Ensure continuity of dates: fill in missing dates with 0 sales\n",
    "        all_dates = pd.date_range(start=disbursement_date - pd.Timedelta(days=sales_period_start), end=disbursement_date - pd.Timedelta(days=sales_period_end))\n",
    "        daily_sales_continuous = pd.DataFrame({'transaction_datetime': all_dates})\n",
    "        daily_sales_continuous = pd.merge(daily_sales_continuous, daily_sales, on='transaction_datetime', how='left').fillna(0)\n",
    "        \n",
    "        # Average of total daily transaction value (in log)\n",
    "        avg_daily_sales = daily_sales_continuous['amount'].mean()\n",
    "        log_avg_transday = np.log1p(avg_daily_sales)\n",
    "        \n",
    "        # Average transaction sizes (in log)\n",
    "        avg_transaction_size = sales_period_transactions['amount'].mean()\n",
    "        log_avgtrans = np.log1p(avg_transaction_size)\n",
    "        \n",
    "        # Coefficient of variation of transaction sizes\n",
    "        std_dev_transaction_size = sales_period_transactions['amount'].std()\n",
    "        cv_trans = std_dev_transaction_size / avg_transaction_size if avg_transaction_size != 0 else 0\n",
    "        \n",
    "        # Coefficient of variation of total daily transaction values\n",
    "        std_dev_daily_sales = daily_sales_continuous['amount'].std()\n",
    "        cv_transday = std_dev_daily_sales / avg_daily_sales if avg_daily_sales != 0 else 0\n",
    "        \n",
    "        # Herfindahl-Hirschmann index of customers total transaction value\n",
    "        customer_sales = sales_period_transactions.groupby('pan')['amount'].sum()\n",
    "        total_sales = customer_sales.sum()\n",
    "        customer_sales_proportion = customer_sales / total_sales\n",
    "        HH_cust_trans = (customer_sales_proportion ** 2).sum()\n",
    "        \n",
    "        # Average of periods without any transactions (# of inactive days/total number of transaction days)\n",
    "        transaction_dates = sales_period_transactions['transaction_datetime'].dt.date.unique()\n",
    "        inactive_days = len(set(all_dates.date) - set(transaction_dates))\n",
    "        total_transaction_days = len(transaction_dates)\n",
    "        avg_inactivity = inactive_days / total_transaction_days if total_transaction_days != 0 else 0\n",
    "        \n",
    "        # Days since the period of longest inactivity\n",
    "        all_dates_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "        all_dates_df['had_transaction'] = all_dates_df['date'].isin(transaction_dates)\n",
    "        all_dates_df['inactive_period'] = (all_dates_df['had_transaction'] == False).astype(int).diff().ne(0).cumsum()\n",
    "        inactive_periods = all_dates_df[all_dates_df['had_transaction'] == False].groupby('inactive_period')['date'].agg(['min', 'max', 'size'])\n",
    "        longest_inactivity_period = inactive_periods.loc[inactive_periods['size'].idxmax()]\n",
    "        days_since_max_inactivity = (disbursement_date.date() - longest_inactivity_period['max'].date()).days\n",
    "        \n",
    "        # Days since last transaction\n",
    "        last_transaction_date = sales_period_transactions['transaction_datetime'].max().date()\n",
    "        days_since_lasttrans = (disbursement_date.date() - last_transaction_date).days\n",
    "        \n",
    "        # (Relative to disbursal) Day with the largest transactions value\n",
    "        largest_transaction_day = daily_sales.loc[daily_sales['daily_sales'].idxmax(), 'transaction_datetime']\n",
    "        max_trans_dt = (largest_transaction_day - disbursement_date.date()).days\n",
    "    \n",
    "        # (Relative to disbursal) Day with most number of transactions\n",
    "        daily_transaction_count = sales_period_transactions.groupby(sales_period_transactions['transaction_datetime'].dt.date).size().reset_index(name='transaction_count')\n",
    "        most_transactions_day = daily_transaction_count.loc[daily_transaction_count['transaction_count'].idxmax(), 'transaction_datetime']\n",
    "        max_transcount_dt = (most_transactions_day - disbursement_date.date()).days\n",
    "        \n",
    "        # Days since last transaction of most frequent customer at the time of disbursal.\n",
    "        most_frequent_customer = sales_period_transactions.groupby('pan').size().idxmax()\n",
    "        most_frequent_customer_transactions = sales_period_transactions[sales_period_transactions['pan'] == most_frequent_customer]\n",
    "        most_frequent_customer_last_transaction_date = most_frequent_customer_transactions['transaction_datetime'].max().date()\n",
    "        dayspast_freqcust = (disbursement_date.date() - most_frequent_customer_last_transaction_date).days\n",
    "        \n",
    "        # Days since last transaction of largest customer at the time of disbursal.\n",
    "        largest_customer = sales_period_transactions.groupby('pan')['amount'].sum().idxmax()\n",
    "        largest_customer_transactions = sales_period_transactions[sales_period_transactions['pan'] == largest_customer]\n",
    "        last_transaction_largest_customer = largest_customer_transactions['transaction_datetime'].max().date()\n",
    "        dayspast_largcust = (disbursement_date.date() - last_transaction_largest_customer).days\n",
    "        \n",
    "        # Number of distinct customers within period (in log).\n",
    "        custcount = sales_period_transactions['pan'].nunique()\n",
    "        log_custcount = np.log1p(custcount)\n",
    "        \n",
    "        # Share of total transaction value conducted on {DayOfWeek}\n",
    "        sales_period_transactions['day_of_week'] = sales_period_transactions['transaction_datetime'].dt.dayofweek\n",
    "        transaction_value_by_day = sales_period_transactions.groupby('day_of_week')['amount'].sum()\n",
    "        total_transaction_value = sales_period_transactions['amount'].sum()\n",
    "        share_by_day_of_week = transaction_value_by_day / total_transaction_value\n",
    "        day_of_week_share = share_by_day_of_week.to_dict()\n",
    "        \n",
    "        shr_Mon_trans = day_of_week_share[0]\n",
    "        shr_Tue_trans = day_of_week_share[1]\n",
    "        shr_Wed_trans = day_of_week_share[2]\n",
    "        shr_Thu_trans = day_of_week_share[3]\n",
    "        shr_Fri_trans = day_of_week_share[4]\n",
    "        shr_Sat_trans = day_of_week_share[5]\n",
    "        shr_Sun_trans = day_of_week_share[6]\n",
    "        \n",
    "        results.append({'merchant_code': merchant_code, f'log_avg_transday{suffix}':log_avg_transday, f'log_avgtrans{suffix}':log_avgtrans, \n",
    "                        f'cv_trans{suffix}':cv_trans, f'cv_transday{suffix}':cv_transday, f'HH_cust_trans{suffix}':HH_cust_trans, \n",
    "                        f'avg_inactivity{suffix}':avg_inactivity, f'days_since_max_inactivity{suffix}':days_since_max_inactivity, \n",
    "                        f'days_since_lasttrans{suffix}':days_since_lasttrans, f'max_trans_dt{suffix}':max_trans_dt, \n",
    "                        f'max_transcount_dt{suffix}':max_transcount_dt, f'dayspast_freqcust{suffix}':dayspast_freqcust, \n",
    "                        f'dayspast_largcust{suffix}':dayspast_largcust, f'log_custcount{suffix}':log_custcount, \n",
    "                        f'shr_Mon_trans{suffix}':shr_Mon_trans, f'shr_Tue_trans{suffix}':shr_Tue_trans, \n",
    "                        f'shr_Wed_trans{suffix}':shr_Wed_trans, f'shr_Thu_trans{suffix}':shr_Thu_trans, \n",
    "                        f'shr_Fri_trans{suffix}':shr_Fri_trans, f'shr_Sat_trans{suffix}':shr_Sat_trans,\n",
    "                        f'shr_Sun_trans{suffix}':shr_Sun_trans, f'avg_daily_sales{suffix}':avg_daily_sales})\n",
    "\n",
    "    trans_char_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "    return trans_char_df\n",
    "\n",
    "# 91 day period (long term - lt)\n",
    "\n",
    "trans_char_df_lt = calculate_trans_char(transactions, sales_period_end=1, sales_period_start=91, suffix='_lt')\n",
    "\n",
    "# 91 - 64 (t_1) and 1 - 28 (t_2) periods to then calculate change over lt\n",
    "\n",
    "trans_char_df_t_1 = calculate_trans_char(transactions, sales_period_end=64, sales_period_start=91, suffix='_t_1')\n",
    "\n",
    "trans_char_df_t_2 = calculate_trans_char(transactions, sales_period_end=1, sales_period_start=28, suffix='_t_2')\n",
    "\n",
    "# Change t_1, t_2 (da - absolute change, dr - relative change, log...da - absolute change in log approximating relative change) \n",
    "\n",
    "trans_char_df_t_1_t_2 = pd.merge(trans_char_df_t_1, trans_char_df_t_2, on='merchant_code', how='left')\n",
    "\n",
    "def calculate_da(trans_char_df_t_1_t_2, da_variables):\n",
    "    for var in da_variables:\n",
    "       trans_char_df_t_1_t_2[f'{var}_da'] = trans_char_df_t_1_t_2[f'{var}_t_1'] - trans_char_df_t_1_t_2[f'{var}_t_2']\n",
    "    result_da = trans_char_df_t_1_t_2[['merchant_code'] + [f'{var}_da' for var in da_variables]]\n",
    "    return result_da\n",
    "\n",
    "def calculate_dr(trans_char_df_t_1_t_2, dr_variables):\n",
    "    for var in dr_variables:\n",
    "       trans_char_df_t_1_t_2[f'{var}_dr'] = ((trans_char_df_t_1_t_2[f'{var}_t_1'] - trans_char_df_t_1_t_2[f'{var}_t_2']) / trans_char_df_t_1_t_2[f'{var}_t_2']) if trans_char_df_t_1_t_2[f'{var}_t_2'] != 0 else 0\n",
    "    result_dr = trans_char_df_t_1_t_2[['merchant_code'] + [f'{var}_da' for var in dr_variables]]\n",
    "    return result_dr\n",
    "\n",
    "da_variables = ['log_avg_transday', 'log_avgtrans', 'log_custcount',\n",
    "                'shr_Mon_trans', 'shr_Tue_trans', 'shr_Wed_trans',\n",
    "                'shr_Thu_trans', 'shr_Fri_trans', 'shr_Sat_trans',\n",
    "                'shr_Sun_trans']\n",
    "\n",
    "dr_variables = ['cv_trans', 'cv_transday', 'HH_cust_trans',\n",
    "                'avg_inactivity', 'days_since_max_inactivity',\n",
    "                'days_since_lasttrans', 'max_trans_dt',\n",
    "                'max_transcount_dt', 'dayspast_freqcust',\n",
    "                'dayspast_largcust']\n",
    "\n",
    "trans_char_da = calculate_da(trans_char_df_t_1_t_2, da_variables)\n",
    "trans_char_dr = calculate_dr(trans_char_df_t_1_t_2, dr_variables)\n",
    "\n",
    "trans_char = pd.merge(trans_char_df_lt, trans_char_da, on='merchant_code', how='outer')\n",
    "trans_char = pd.merge(trans_char, trans_char_dr, on='merchant_code', how='outer')\n",
    "\n",
    "trans_char.drop('avg_daily_sales_lt', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a> Loan-Sales Ratios  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan-sales ratio. Loan amount divided by average daily sales calculated in the 91-day period before disbursal.\n",
    "\n",
    "loan_sales_ratio_df = pd.merge(loan_char, trans_char_df_lt[['merchant_code', 'avg_daily_sales_lt']], on='merchant_code', how='left')\n",
    "loan_sales_ratio_df['lsratio'] = loan_sales_ratio_df['loanamount'] / loan_sales_ratio_df['avg_daily_sales_lt']\n",
    "\n",
    "# Loan-sales ratio calculated with average daily sales calculated in the 28-day period before disbursal.\n",
    "\n",
    "loan_sales_ratio_df = pd.merge(loan_sales_ratio_df, trans_char_df_t_2[['merchant_code', 'avg_daily_sales_t_2']], on='merchant_code', how='left')\n",
    "loan_sales_ratio_df['rtsshort'] = loan_sales_ratio_df['loanamount'] / loan_sales_ratio_df['avg_daily_sales_lt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a> Target Variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a> Data Exploration  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "dataset['merchant_code'] = merchants['merchant_code']\n",
    "dataset = pd.merge(dataset, loan_sales_ratio_df[['merchant_code', 'lsratio', 'rtsshort']], on='merchant_code', how='left')\n",
    "dataset = pd.merge(dataset, trans_char, on='merchant_code', how='left')\n",
    "dataset = pd.merge(dataset, loan_char, on='merchant_code', how='left')\n",
    "dataset = pd.merge(dataset, merch_char, on='merchant_code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting whole dataset information:\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of unique values in each column\n",
    "print(dataset.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a> Checking/Handling Missing Values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info = dataset.isnull().sum()\n",
    "print(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a> Checking/Removing Outliers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting statistical information of the dataset\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Check Correlation of The Features with the Target Feature i.e. Loan Status\n",
    "corr_matrix=dataset.corr()\n",
    "plt.figure(figsize=(25, 25))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "best_features=corr_matrix.index[abs(corr_matrix['default'])>0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the target variable\n",
    "target = 'default'\n",
    "class_counts = dataset[target].value_counts()\n",
    "plt.bar([0,1], class_counts.values,width=0.3)\n",
    "plt.title('Bar Plot of ' + target)\n",
    "plt.xlabel(target)\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVER-sample the majority class\n",
    "\n",
    "majority_class = class_counts.idxmax()\n",
    "n_samples = class_counts[majority_class]\n",
    "over_sampled_dataset = dataset.groupby('Loan Status').apply(lambda x: x.sample(n_samples, replace=True)).reset_index(drop=True)\n",
    "\n",
    "print('Original dataset shape is', dataset.shape)\n",
    "print('Over-sampled dataset shape is ', over_sampled_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = over_sampled_dataset[target].value_counts()\n",
    "plt.bar([0,1], class_counts.values,width=0.3)\n",
    "plt.title('Bar Plot of ' + target)\n",
    "plt.xlabel(target)\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution\n",
    "columns=over_sampled_dataset.columns.tolist()\n",
    "for column in over_sampled_dataset:\n",
    "  sns.displot(over_sampled_dataset[columns], kde=True, x=column, color=\"red\", edgecolor=\"purple\", linewidth=5, bins=int(180/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "for column in dataset:\n",
    "    over_sampled_dataset[[column]].boxplot(boxprops=dict(color='red'))\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y\n",
    "X = over_sampled_dataset.iloc[:, :-1]  \n",
    "y = over_sampled_dataset.iloc[:, -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Outliers through IQR Method\n",
    "cols = []\n",
    "\n",
    "print(\"Skewness of \")\n",
    "print(\"Before Removing Outliers\")\n",
    "for column in cols:\n",
    "  print(column, X[column].skew())\n",
    "  Q1 = X[column].quantile(0.25)\n",
    "  Q3 = X[column].quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "  upper_limit = Q3 + IQR * 1.5\n",
    "  lower_limit = Q1 - IQR * 1.5\n",
    "  X[column] = np.where(\n",
    "        X[column] > upper_limit,\n",
    "        upper_limit,\n",
    "        np.where(\n",
    "            X[column] < lower_limit,\n",
    "            lower_limit,\n",
    "            X[column]\n",
    "        )\n",
    "    )\n",
    "# Checking Skewness of the indicated columns in cols after removing outliers\n",
    "print(\"After Removing Outliers\")\n",
    "for column in cols:\n",
    "  print(column,X[column].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot after handling outliers\n",
    "for column in X:\n",
    "    X[[column]].boxplot(boxprops=dict(color='red'))\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a> Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment Footprint Basic Model\n",
    "\n",
    "pf_basic = pd.DataFrame()\n",
    "pf_basic['merchant_code'] = merchants['merchant_code']\n",
    "\n",
    "pf_basic = pd.merge(pf_basic, loan_sales_ratio_df[['merchant_code', 'lsratio', 'rtsshort']], on='merchant_code', how='left')\n",
    "pf_basic = pd.merge(pf_basic, trans_char[['merchant_code', 'log_avgtrans_lt', 'log_avg_transday_lt', 'cv_trans_lt', 'cv_transday_lt''log_avgtrans_da', 'log_avg_transday_da', 'cv_trans_dr', 'cv_transday_dr']], on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Deep Model\n",
    "\n",
    "pf_deep = pd.DataFrame()\n",
    "pf_deep['merchant_code'] = merchants['merchant_code']\n",
    "\n",
    "pf_deep = pd.merge(pf_deep, loan_sales_ratio_df[['merchant_code', 'lsratio', 'rtsshort']], on='merchant_code', how='left')\n",
    "pf_deep = pd.merge(pf_deep, trans_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Basic Model + Loan Char\n",
    "\n",
    "pf_basic_lc = pd.merge(pf_basic, loan_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Deep Model + Loan Char\n",
    "\n",
    "pf_deep_lc = pd.merge(pf_deep, loan_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Basic Model + Merchant Char\n",
    "\n",
    "pf_basic_mc = pd.merge(pf_basic, merch_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Deep Model + Merchant Char\n",
    "\n",
    "pf_deep_mc = pd.merge(pf_deep, loan_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Basic Model + Loan Char + Merchant Char\n",
    "\n",
    "pf_basic_lc_mc = pd.merge(pf_basic_lc, merch_char, on='merchant_code', how='left')\n",
    "\n",
    "# Payment Footprint Deep Model + Loan Char + Merchant Char\n",
    "\n",
    "pf_deep_lc_mc = pd.merge(pf_deep_lc, merch_char, on='merchant_code', how='left')\n",
    "\n",
    "models = [\n",
    "    ('pf_basic', pf_basic),\n",
    "    ('pf_deep', pf_deep),\n",
    "    ('pf_basic_lc', pf_basic_lc),\n",
    "    ('pf_deep_lc', pf_deep_lc),\n",
    "    ('pf_basic_mc', pf_basic_mc),\n",
    "    ('pf_deep_mc', pf_deep_mc),\n",
    "    ('pf_basic_lc_mc', pf_basic_lc_mc),\n",
    "    ('pf_deep_lc_mc', pf_deep_lc_mc)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a> Splitting Dataset into Training and Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a> Model Selection and Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_1_'></a> XGBoost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(xtrain, label=ytrain) #Converting data into DMatrix format, which is the input format required by XGBoost\n",
    "dtest = xgb.DMatrix(xtest, label=ytest)\n",
    "params = { # Specifying parameters\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"max_depth\": 3,\n",
    "    \"eta\": 0.1,\n",
    "    \"gamma\": 0.1,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"verbosity\": 0\n",
    "}\n",
    "num_of_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_of_rounds) # Training\n",
    "y_pred = model.predict(dtest) # Prediction\n",
    "y_train_pred = model.predict(dtrain) # Prediction\n",
    "train_acc= accuracy_score(ytrain, y_train_pred.round()) # Training\n",
    "acc = accuracy_score(ytest, y_pred.round()) # Calculating accuracy \n",
    "print(f\"Training Accuracy: {train_acc}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(classification_report(ytest, y_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a> Decision Tree and Random Forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "\n",
    "model_list = [rf,dt]\n",
    "test_acc = []\n",
    "train_acc=[]\n",
    "for i in model_list:\n",
    "    i_model = i.fit(xtrain,ytrain)\n",
    "    ypred_train = i_model.predict(xtrain)\n",
    "    ypred_test = i_model.predict(xtest)\n",
    "    print(i)\n",
    "    print(classification_report(ytest, ypred_test))\n",
    "    print(f1_score(ytest, ypred_test, average='macro'))\n",
    "    print(f1_score(ytest, ypred_test, average='micro'))\n",
    "    train_acc.append(accuracy_score(ytrain,ypred_train))\n",
    "    test_acc.append(accuracy_score(ytest,ypred_test))\n",
    "print(\"Training Accuracy: \",train_acc)\n",
    "print(\"Testing Accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a> Hypertuning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and their possible values\n",
    "hyperparameters = {\n",
    "    'max_depth': [None,36,40,44],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "Decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Grid Search for Decision Trees:\n",
    "grid_search_dt = GridSearchCV(Decision_tree, hyperparameters, cv=4)\n",
    "grid_search_dt.fit(xtrain, ytrain)\n",
    "\n",
    "# Best parameters and score\n",
    "print('Best Parameters:', grid_search_dt.best_params_)\n",
    "# Validation score\n",
    "print('Best validation Score:', grid_search_dt.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_4_'></a> Notes:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Deployment\n",
    "    - filename = 'DeploymentModel.joblib'\n",
    "    - joblib.dump({Final Model}, filename)\n",
    "- Target var\n",
    "- Run a for loop for each pf_... dataset\n",
    "- Edit pf_... to include balanced data\n",
    "- Consider K-Fold Cross-Validation or multiple splits with different random seeds\n",
    "- Consider different models (neural network, logit regression)\n",
    "- Early warning exercise\n",
    "\n",
    "Target Vars:\n",
    "\n",
    "Default - binary variable that takes value 1, if the loan had a shortfall > 5% of repayment amount and it was either written off or still pending\n",
    "\n",
    "Late - binary variable that takes value 1 if a loan was non-defaulting and took at least 30 days more than the implied tenure to fully pay the loan off\n",
    "\n",
    "Non-performing - binary variable that takes value 1 when either the loan is categorized as Default or Late\n",
    "\n",
    "N.B. Implied tenure is the number of days it would take to repay the loan (loan amount + interest) given the 10% deduction rate and if the merchant after disbursal continued to transact at their pre-disbursal long-term average. Pre-disbursal long-term average is average daily transaction value calculated over 91 days between day 120 and day 30 before loan disbursal.\n",
    "\n",
    "Variables:\n",
    "\n",
    "merchant_code - Merchant Identifier\n",
    "\n",
    "relationage - Age of relationship with the payment company at the time of disbursal (in months)\n",
    "\n",
    "mdisburse - Month of loan disbursal\n",
    "\n",
    "loanamount - Principal amount\n",
    "\n",
    "lsratio - Loan-sales ratio. Loan amount divided by average daily sales calculated in the 91-day period before disbursal.\n",
    "\n",
    "rtsshort - Loan-sales ratio calculated with average daily sales calculated in the 28-day period before disbursal.\n",
    "\n",
    "log_avg_transday - Average of total daily transaction value (in log).\n",
    "\n",
    "log_avgtrans - Average transaction sizes (in log).\n",
    "\n",
    "log_custcount - Number of distinct customers within period (in log).\n",
    "\n",
    "cv_trans - Coefficient of variation of transaction sizes.\n",
    "\n",
    "cv_transday - Coefficient of variation of total daily transaction values.\n",
    "\n",
    "HH_cust_trans - Herfindahl-Hirschmann index of customers total transaction value.\n",
    "\n",
    "avg_inactivity - Average of periods without any transactions (# of inactive days/total number of transaction days)\n",
    "\n",
    "days_since_max_inactivity - Days since the period of longest inactivity\n",
    "\n",
    "days_since_lasttrans - Days since last transaction\n",
    "\n",
    "max_trans_dt - (Relative to disbursal) Day with the largest transactions value.\n",
    "\n",
    "max_transcount_dt - (Relative to disbursal) Day with most number of transactions.\n",
    "\n",
    "dayspast_freqcust - Days since last transaction of most frequent customer at the time of disbursal.\n",
    "\n",
    "dayspast_largcust -  Days since last transaction of largest customer at the time of disbursal.\n",
    "\n",
    "shr_Mon_trans - Share of total transaction value conducted on Mondays\n",
    "\n",
    "shr_Tue_trans - Share of total transaction value conducted on Tuesdays\n",
    "\n",
    "shr_Wed_trans - Share of total transaction value conducted on Wednesdays\n",
    "\n",
    "shr_Thu_trans - Share of total transaction value conducted on Thursdays\n",
    "\n",
    "shr_Fri_trans - Share of total transaction value conducted on Fridays\n",
    "\n",
    "shr_Sat_trans - Share of total transaction value conducted on Saturdays\n",
    "\n",
    "shr_Sun_trans - Share of total transaction value conducted on Sundays\n",
    "\n",
    "N.B.\n",
    "Variables ending with subscript \"_lh\" are calculated in the 91-day window before disbursal.\n",
    "Variable ending with \"_dr\" and \"_da\" represent relative and absolute changes in values of the payment variables calculated in the 28-day period before disbursal and values calculated in the 63-day to 91-day period before disbursal.\n",
    "Absolute changes in log variables approximate the relative change in the variable.\n",
    "\n",
    "Variables to add:\n",
    "\n",
    "shr_AMEX_trans_lh \n",
    "shr_MAESTRO_trans_lh \n",
    "shr_MASTERCARDVISA_trans_lh \n",
    "shr_OTHER_trans_lh \n",
    "shr_fallbacktrans_lh \n",
    "lastfallback_days_lh \n",
    "lastfallback_amt_relavg_lh \n",
    "shr_refundtrans_lh\n",
    "++ shr card types, shr switch name?, demographic info (location, age, gender, etc?)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
